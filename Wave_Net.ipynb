{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa820724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d755da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "context_size = 256\n",
    "num_epochs = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cpu'\n",
    "eval_iterations = 200\n",
    "d_model = 20\n",
    "d_hidden = 100\n",
    "n_layer = 1\n",
    "dropout = 0.2\n",
    "write_to_file = False\n",
    "norm = 'batch_norm'\n",
    "\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7edfc454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "with open('data/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45dac42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the unique characters in the text.                                                                                                             \n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from a character to a text.                                                                                                                            \n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "# encode: is a lambda function that takes a string and returns  a list of ints, where each character is mapped to the right int.\n",
    "encode = lambda s: [char_to_idx[c] for c in s]\n",
    "# decode: is the reverse mapping of encode. It takes a list of int, and returns a string.\n",
    "decode = lambda l: ''.join([idx_to_char[i] for i in l]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "983d44c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a train-test split with 90% of the data train and 10% test.\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "train_data_size = int(0.9 * len(data))\n",
    "train_data = data[:train_data_size]\n",
    "val_data = data[train_data_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2eaf331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data.\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    indices = torch.randint(len(data) - context_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_size] for i in indices])\n",
    "    y = torch.stack([data[i+context_size] for i in indices])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e09f1c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNetModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        self.linear_layers = []\n",
    "        self.norm_layers = []\n",
    "        \n",
    "        temp_context_size = context_size\n",
    "        \n",
    "        while temp_context_size >= 10:\n",
    "            if not self.linear_layers:\n",
    "                self.linear_layers.append(nn.Linear(2 * d_model, d_hidden))\n",
    "                self.norm_layers.append(nn.BatchNorm1d(d_hidden))\n",
    "            else:\n",
    "                self.linear_layers.append(nn.Linear(2 * d_hidden, d_hidden))\n",
    "                self.norm_layers.append(nn.BatchNorm1d(d_hidden))\n",
    "            \n",
    "            temp_context_size //= 2\n",
    "            \n",
    "        self.output_norm = nn.BatchNorm1d(vocab_size)\n",
    "        self.output_linear = nn.Linear(temp_context_size * d_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        N, T = idx.shape\n",
    "\n",
    "        token_embeddings = self.token_embedding(idx)\n",
    "        \n",
    "        x = token_embeddings\n",
    "        \n",
    "        for i, _ in enumerate(self.linear_layers):\n",
    "            N, T, D = x.shape\n",
    "            x = x.contiguous().view(N, T // 2, -1)\n",
    "            x = self.linear_layers[i](x)\n",
    "            x = x.transpose(-2, -1)\n",
    "            x = self.norm_layers[i](x)\n",
    "            x = x.transpose(-2, -1)\n",
    "            x = nn.ReLU()(x)\n",
    "            \n",
    "        x = x.contiguous().view(N, -1)\n",
    "        \n",
    "        x = nn.Dropout(dropout)(x)\n",
    "        \n",
    "        x = self.output_linear(x)\n",
    "                \n",
    "        x = self.output_norm(x)\n",
    "\n",
    "        logits = nn.Tanh()(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            _, T = logits.shape\n",
    "\n",
    "            assert(T == vocab_size)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -context_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "643033f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.053495 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = WaveNetModel().to(device)\n",
    "# Print the number of parameters in the model.\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer                                                                                                                                                                                                                                        \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f7599c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the loss.\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iterations)\n",
    "        for k in range(eval_iterations):\n",
    "            x, y = get_batch(split)\n",
    "            logits, loss = model(x, y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c03fa4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2725, val loss 4.2682\n",
      "step 500: train loss 4.1988, val loss 4.2146\n",
      "step 1000: train loss 4.0499, val loss 4.0618\n",
      "step 1500: train loss 3.9044, val loss 3.9278\n",
      "step 2000: train loss 3.7916, val loss 3.8184\n",
      "step 2500: train loss 3.7078, val loss 3.7338\n",
      "step 3000: train loss 3.6448, val loss 3.6677\n",
      "step 3500: train loss 3.6081, val loss 3.6383\n",
      "step 4000: train loss 3.5770, val loss 3.6072\n",
      "step 4500: train loss 3.5561, val loss 3.5843\n",
      "step 4999: train loss 3.5361, val loss 3.5624\n"
     ]
    }
   ],
   "source": [
    "# loop over max_iters and at each iter we get a batch of data we optimize over.\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets                                                                                                                                                                                                 \n",
    "    if epoch % eval_interval == 0 or epoch == num_epochs - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data                                                                                                                                                                                                                                        \n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss                                                                                                                                                                                                                                             \n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8115e4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "aoRhSvdwNbht-vk;n$uv-FfAu-3\n",
      "\n",
      "y?\n",
      "o\n",
      "hnrecQCrHua&.MQl\n",
      "YVDk dy;inblloE\n",
      "Vn$3otBst&i:mY'dqhpNgnt:fP '-tGl\n",
      "unn-WhmspMSleGrwVyIksffajfaVerMs ao&evn h,3hBdi\n",
      "sea NBs? hQMeEeoaV?;ilI heoFXBa,baW,- \n",
      "cm!e!wg fabZofceoQo mksoxnNelipUaFiraoRhuGsorl:nnwiuXA,Y!n HwYOFfchF'nkhicamrCglh &SxRnfIYhyo-LjibonPmoeMfN lzyHcBoiitOnDTUAa\n",
      ":eYXhnne?gs;P,hka Ywlpadrw:SrdMey!f;hmrcsJGOa NLhektyQVeeh-XJknGws$WH doYWfv\n",
      "rVdFTyePQ\n",
      "v:DnhbjosAnioeae,uPfcv\n",
      "is CmCs'YTe, aCam?t'.w&JnhLb O\n",
      "muIi$Ge,FgxseZicibmymaoZh,dDg C:aUibVwM'QIMGd,\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "context = train_data[:256].reshape(1, 256)\n",
    "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n",
    "if write_to_file:\n",
    "    open('wave_net.txt', 'w').write(decode(model.generate(context, max_new_tokens=10000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
