{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f13ded9f",
      "metadata": {
        "id": "f13ded9f"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import zipfile\n",
        "from matplotlib import pylab\n",
        "from six.moves import range\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torchtext.data.utils import get_tokenizer, ngrams_iterator\n",
        "from torchtext.datasets import DATASETS\n",
        "from torchtext.utils import download_from_url\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import torch.nn as nn\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "from torchtext.vocab import FastText, CharNGram\n",
        "from itertools import chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4eea599c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eea599c",
        "outputId": "680df7e6-4ffc-481d-c1dd-150368ccc460"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found and verified data/conllpp_train.txt\n",
            "Found and verified data/conllpp_dev.txt\n",
            "Found and verified data/conllpp_test.txt\n"
          ]
        }
      ],
      "source": [
        "# Download the data\n",
        "\n",
        "DATA_URL = 'https://github.com/ZihanWangKi/CrossWeigh/raw/master/data/'\n",
        "DATA_DIR = 'data'\n",
        "\n",
        "def download_file(url, filename, data_dir, expected_bytes):\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "    \n",
        "    file_path = os.path.join(data_dir, filename)\n",
        "    if not os.path.exists(file_path):\n",
        "        file_path, _ = urlretrieve(url + filename, file_path)\n",
        "    else:\n",
        "        file_path = os.path.join(data_dir, filename)\n",
        "    \n",
        "    stat_info = os.stat(file_path)\n",
        "    if stat_info.st_size == expected_bytes:\n",
        "        print(f'Found and verified {file_path}')\n",
        "    else:\n",
        "        print(stat_info.st_size)\n",
        "        raise Exception(f'Failed to verify {file_path}. Can you retrieve it with a browser?')\n",
        "        \n",
        "    return file_path\n",
        "\n",
        "train_file = download_file(DATA_URL, 'conllpp_train.txt', DATA_DIR, 3283420)\n",
        "dev_file = download_file(DATA_URL, 'conllpp_dev.txt', DATA_DIR, 827443)  \n",
        "test_file = download_file(DATA_URL, 'conllpp_test.txt', DATA_DIR, 748737)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "dc3af9ed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc3af9ed",
        "outputId": "71fb8fbe-ba5d-4743-d5f9-d631d08b330f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-DOCSTART- -X- -X- O\n",
            "\n",
            "EU NNP B-NP B-ORG\n",
            "rejects VBZ B-VP O\n",
            "German JJ B-NP B-MISC\n",
            "call NN I-NP O\n",
            "to TO B-VP O\n",
            "boycott VB I-VP O\n",
            "British JJ B-NP B-MISC\n",
            "lamb NN I-NP O\n"
          ]
        }
      ],
      "source": [
        "!head data/conllpp_train.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4db2c06f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4db2c06f",
        "outputId": "2f696b31-864f-4629-ec5d-f059e3b5659b"
      },
      "outputs": [],
      "source": [
        "def load_data(file_path):\n",
        "    print(\"Loading data...\")\n",
        "    sentences, labels = [], []\n",
        "    \n",
        "    with open(file_path, 'r', encoding='latin-1') as file:\n",
        "        sentence_start = True\n",
        "        sentence_tokens = []\n",
        "        sentence_labels = []\n",
        "        \n",
        "        for line in file:\n",
        "            if len(line.strip()) == 0 or line.split(' ')[0] == '-DOCSTART-':\n",
        "                sentence_start = False\n",
        "            else:\n",
        "                sentence_start = True\n",
        "                token, _, _, label = line.split(' ')\n",
        "                sentence_tokens.append(token)\n",
        "                sentence_labels.append(label.strip())\n",
        "            \n",
        "            if not sentence_start and len(sentence_tokens) > 0:\n",
        "                sentences.append(' '.join(sentence_tokens))\n",
        "                labels.append(sentence_labels)\n",
        "                sentence_tokens, sentence_labels = [], []\n",
        "    \n",
        "    print('\\tDone')\n",
        "    return sentences, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cfe6e329",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "\tDone\n",
            "Loading data...\n",
            "\tDone\n",
            "Loading data...\n",
            "\tDone\n",
            "Train size: 14041\n",
            "Dev size: 3250\n",
            "Test size: 3452\n",
            "\n",
            "Sample data\n",
            "\n",
            "Sentence: CRICKET - LEICESTERSHIRE TAKE OVER AT TOP AFTER INNINGS VICTORY .\n",
            "Labels: ['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "\n",
            "\n",
            "Sentence: LONDON 1996-08-30\n",
            "Labels: ['B-LOC', 'O']\n",
            "\n",
            "\n",
            "Sentence: West Indian all-rounder Phil Simmons took four for 38 on Friday as Leicestershire beat Somerset by an innings and 39 runs in two days to take over at the head of the county championship .\n",
            "Labels: ['B-MISC', 'I-MISC', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "\n",
            "\n",
            "Sentence: Their stay on top , though , may be short-lived as title rivals Essex , Derbyshire and Surrey all closed in on victory while Kent made up for lost time in their rain-affected match against Nottinghamshire .\n",
            "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O']\n",
            "\n",
            "\n",
            "Sentence: After bowling Somerset out for 83 on the opening morning at Grace Road , Leicestershire extended their first innings by 94 runs before being bowled out for 296 with England discard Andy Caddick taking three for 83 .\n",
            "Labels: ['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O']\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_sentences, train_labels = load_data(train_file)\n",
        "dev_sentences, dev_labels = load_data(dev_file)\n",
        "test_sentences, test_labels = load_data(test_file)\n",
        "# Print some stats\n",
        "print(f\"Train size: {len(train_labels)}\")\n",
        "print(f\"Dev size: {len(dev_labels)}\")\n",
        "print(f\"Test size: {len(test_labels)}\")\n",
        "\n",
        "# Print some data\n",
        "print('\\nSample data\\n')\n",
        "for sent, labels in zip(dev_sentences[:5], dev_labels[:5]):\n",
        "    print(f\"Sentence: {sent}\")\n",
        "    print(f\"Labels: {labels}\")\n",
        "    assert(len(sent.split(' ')) == len(labels))\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f25d7ec1",
      "metadata": {
        "id": "f25d7ec1"
      },
      "outputs": [],
      "source": [
        "class SentenceTokenizer:\n",
        "    def __call__(self, sentence):\n",
        "        return sentence.lower().split(' ')\n",
        "    \n",
        "class WordTokenizer:\n",
        "    def __call__(self, word):\n",
        "        return [c for c in word.lower()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "zWiq0llwObJ4",
      "metadata": {
        "id": "zWiq0llwObJ4"
      },
      "outputs": [],
      "source": [
        "sentence_tokenizer = SentenceTokenizer()\n",
        "word_tokenizer = WordTokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f77d3f20",
      "metadata": {},
      "outputs": [],
      "source": [
        "sentences = train_sentences + test_sentences + dev_sentences\n",
        "all_labels = train_labels + test_labels + dev_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2eee65e3",
      "metadata": {
        "id": "2eee65e3"
      },
      "outputs": [],
      "source": [
        "def yield_word_tokens(data):\n",
        "    for sentence in data:\n",
        "        yield sentence_tokenizer(sentence)\n",
        "        \n",
        "def yield_char_tokens(data):\n",
        "    for word_tokens in yield_word_tokens(data):\n",
        "        for word_token in word_tokens:\n",
        "            yield word_tokenizer(word_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b2861dab",
      "metadata": {
        "id": "b2861dab"
      },
      "outputs": [],
      "source": [
        "word_vocab = build_vocab_from_iterator(yield_word_tokens(sentences), specials=('<pad>', '<unk>'))\n",
        "char_vocab = build_vocab_from_iterator(yield_char_tokens(sentences), specials=('<pad>', '<unk>'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f5dc348d",
      "metadata": {
        "id": "f5dc348d"
      },
      "outputs": [],
      "source": [
        "# Get the word to idx and idx to char dictionaries\n",
        "word_to_idx = word_vocab.get_stoi()\n",
        "idx_to_word = word_vocab.get_itos()\n",
        "# Get the char to idx and idx to char dictionaries\n",
        "char_to_idx = char_vocab.get_stoi()\n",
        "idx_to_char = char_vocab.get_itos()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d81a25c9",
      "metadata": {
        "id": "d81a25c9"
      },
      "outputs": [],
      "source": [
        "def get_label_mappings(labels):\n",
        "    unique_labels = pd.Series(chain(*labels)).unique()\n",
        "    label_to_idx = dict(zip(unique_labels, np.arange(unique_labels.shape[0])))\n",
        "    idx_to_label = {i: label for label, i in label_to_idx.items()}\n",
        "    \n",
        "    label_weights = {}\n",
        "    label_counts = pd.Series(chain(*labels)).value_counts()\n",
        "    \n",
        "    for label, count in label_counts.items():\n",
        "        label_weights[label_to_idx[label]] = label_counts.min() / count\n",
        "    \n",
        "    return label_to_idx, idx_to_label, label_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "fa3b9fc5",
      "metadata": {
        "id": "fa3b9fc5"
      },
      "outputs": [],
      "source": [
        "label_to_idx, idx_to_label, label_weights = get_label_mappings(train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "KMsQOeBSguWx",
      "metadata": {
        "id": "KMsQOeBSguWx"
      },
      "outputs": [],
      "source": [
        "for label, idx in label_to_idx.items():\n",
        "    assert(label == idx_to_label[idx])\n",
        "    assert(idx in label_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "96393bb8",
      "metadata": {
        "id": "96393bb8"
      },
      "outputs": [],
      "source": [
        "# Get the weights per class as a tensor\n",
        "class_weights = torch.zeros(len(label_weights))\n",
        "for i, weight in label_weights.items():\n",
        "    class_weights[i] = weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "4b964cf9",
      "metadata": {
        "id": "4b964cf9"
      },
      "outputs": [],
      "source": [
        "labels = pd.Series(chain(*train_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "r1hus59XV0Fy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1hus59XV0Fy",
        "outputId": "fb309794-c0b9-483f-d974-e844025911bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training label counts:\n",
            "O         169578\n",
            "B-LOC       7140\n",
            "B-PER       6600\n",
            "B-ORG       6321\n",
            "I-PER       4528\n",
            "I-ORG       3704\n",
            "B-MISC      3438\n",
            "I-LOC       1157\n",
            "I-MISC      1155\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Validation label counts:\n",
            "O         42759\n",
            "B-PER      1842\n",
            "B-LOC      1837\n",
            "B-ORG      1341\n",
            "I-PER      1307\n",
            "B-MISC      922\n",
            "I-ORG       751\n",
            "I-MISC      346\n",
            "I-LOC       257\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Test label counts:\n",
            "O         38143\n",
            "B-ORG      1714\n",
            "B-LOC      1645\n",
            "B-PER      1617\n",
            "I-PER      1161\n",
            "I-ORG       881\n",
            "B-MISC      722\n",
            "I-LOC       259\n",
            "I-MISC      252\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Check for class balance\n",
        "\n",
        "print(\"Training label counts:\")  \n",
        "print(pd.Series(chain(*train_labels)).value_counts())\n",
        "\n",
        "print(\"\\nValidation label counts:\")\n",
        "print(pd.Series(chain(*dev_labels)).value_counts())\n",
        "\n",
        "print(\"\\nTest label counts:\")  \n",
        "print(pd.Series(chain(*test_labels)).value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "eb6a72e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb6a72e7",
        "outputId": "a2cb0c89-a262-46a0-d3bd-207bc0bf2571"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    14041.000000\n",
              "mean        14.501887\n",
              "std         11.602756\n",
              "min          1.000000\n",
              "5%           2.000000\n",
              "50%         10.000000\n",
              "95%         37.000000\n",
              "max        113.000000\n",
              "dtype: float64"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Series length\n",
        "pd.Series(train_sentences).str.split().str.len().describe(percentiles=[0.05, 0.95])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec9ef51e",
      "metadata": {
        "id": "ec9ef51e"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "dc9dad21",
      "metadata": {
        "id": "dc9dad21"
      },
      "outputs": [],
      "source": [
        "# Size of token embeddings\n",
        "EMBEDDING_DIM = 300\n",
        "# Number of hidden units in the GRU layer\n",
        "HIDDEN_DIM = 64\n",
        "# Number of hidden units in the GRU layer\n",
        "CHAR_DIM = 32\n",
        "# Number of output nodes in the last layer\n",
        "NUM_CLASSES = len(idx_to_label)\n",
        "\n",
        "BATCH_SIZE = 128  \n",
        "EPOCHS = 25\n",
        "FAST_TEXT = FastText(\"simple\")\n",
        "LEARNING_RATE = 1.0\n",
        "MAX_WORD_LENGTH = 12\n",
        "\n",
        "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "0a141dd7",
      "metadata": {
        "id": "0a141dd7"
      },
      "outputs": [],
      "source": [
        "def collate_batch(batch):\n",
        "    label_list, sentence_list, sentence_lengths = [], [], []\n",
        "    word_list = []\n",
        "\n",
        "    for sentence, words, labels in batch:\n",
        "        sentence_list.append(torch.tensor(sentence, dtype=torch.int64))\n",
        "        sentence_lengths.append(len(sentence))\n",
        "        label_list.append(torch.tensor(labels, dtype=torch.int64))\n",
        "        word_list.append(torch.tensor(words, dtype=torch.int64))\n",
        "            \n",
        "    return (\n",
        "        nn.utils.rnn.pad_sequence(sentence_list, batch_first=True).to(DEVICE),\n",
        "        nn.utils.rnn.pad_sequence(label_list, batch_first=True, padding_value=-1).to(DEVICE),    \n",
        "        torch.tensor(sentence_lengths).to(DEVICE),\n",
        "        nn.utils.rnn.pad_sequence(word_list, batch_first=True).to(DEVICE)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "cfda3844",
      "metadata": {
        "id": "cfda3844"
      },
      "outputs": [],
      "source": [
        "def get_dataloader(sentences, labels):\n",
        "    data = []\n",
        "\n",
        "    for sentence, labels in zip(sentences, labels):\n",
        "        word_tokens = sentence_tokenizer(sentence)\n",
        "        int_sentence = word_vocab(word_tokens)\n",
        "        int_words = []\n",
        "        for word_token in word_tokens:\n",
        "            int_words.append(char_vocab(word_tokenizer(word_token[:MAX_WORD_LENGTH]) + max(0, MAX_WORD_LENGTH - len(word_token)) * ['<pad>']))\n",
        "                    \n",
        "        labels = [label_to_idx[label] for label in labels]\n",
        "        assert(len(int_sentence) == len(labels))\n",
        "        data.append([int_sentence, int_words, labels])\n",
        "        \n",
        "    return DataLoader(data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "8c0f1dad",
      "metadata": {
        "id": "8c0f1dad"
      },
      "outputs": [],
      "source": [
        "train_dataloader = get_dataloader(train_sentences, train_labels)\n",
        "dev_dataloader = get_dataloader(dev_sentences, dev_labels)\n",
        "test_dataloader = get_dataloader(test_sentences, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "9b308741",
      "metadata": {
        "id": "9b308741"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "\n",
        "class NERModel(nn.Module):\n",
        "    def __init__(self, num_classes, embedding_dim, hidden_dim, initialize=True, fine_tune_embeddings=True, use_conv_embeddings=True):\n",
        "        super(NERModel, self).__init__()\n",
        "        self.vocab_size = len(word_vocab)\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.char_dim = 32\n",
        "        self.kernel_size = 5\n",
        "        self.max_word_length = MAX_WORD_LENGTH\n",
        "        self.use_conv_embeddings = use_conv_embeddings\n",
        "        \n",
        "        if self.use_conv_embeddings:\n",
        "            self.conv = nn.Conv1d(self.char_dim, self.char_dim, self.kernel_size)\n",
        "            self.max_pool = nn.MaxPool1d(self.max_word_length - self.kernel_size + 1)\n",
        "            \n",
        "        self.embedding = nn.Embedding(len(word_vocab), embedding_dim if not initialize else 300, padding_idx=0)\n",
        "        \n",
        "        self.char_embedding = nn.Embedding(len(char_vocab), self.char_dim, padding_idx=0)\n",
        "        \n",
        "        if initialize:\n",
        "            self.embedding.weight.requires_grad = False\n",
        "            for i in range(len(word_vocab)):\n",
        "                token = word_vocab.lookup_token(i)\n",
        "                self.embedding.weight[i, :] = FAST_TEXT.get_vecs_by_tokens(token, lower_case_backup=True)\n",
        "            self.embedding.weight.requires_grad = True\n",
        "        else:\n",
        "            self.init_weights()\n",
        "                \n",
        "        if not fine_tune_embeddings:\n",
        "            self.embedding.weight.requires_grad = False\n",
        "        \n",
        "        self.rnn = nn.GRU(self.embedding_dim + self.char_dim, self.hidden_dim, batch_first=True, bidirectional=True)\n",
        "\n",
        "        self.fc = nn.Linear(2 * self.hidden_dim, num_classes)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        if self.use_conv_embeddings:\n",
        "            self.char_embedding.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, sentences, lengths, words):\n",
        "        embedded_sentences = self.embedding(sentences.int()) \n",
        "        \n",
        "        if self.use_conv_embeddings:                        \n",
        "            embedded_words = self.char_embedding(words.int())\n",
        "                                                \n",
        "            N, L_sentence, L_word, D_char = embedded_words.shape\n",
        "            \n",
        "            embedded_words = embedded_words.view(N * L_sentence, L_word, -1)\n",
        "            embedded_words = torch.swapaxes(embedded_words, 2, 1)\n",
        "            embedded_words = self.conv(embedded_words)\n",
        "            embedded_words = self.max_pool(embedded_words).squeeze()\n",
        "            embedded_words = embedded_words.view(N, L_sentence, -1)\n",
        "                      \n",
        "            embedded_sentences = torch.cat([embedded_sentences, embedded_words], axis=-1)\n",
        "            \n",
        "        embedded_sentences = nn.utils.rnn.pack_padded_sequence(embedded_sentences, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True)\n",
        "        \n",
        "        logits, _ = self.rnn(embedded_sentences)\n",
        "         \n",
        "        logits, _ = nn.utils.rnn.pad_packed_sequence(logits, batch_first=True)\n",
        "\n",
        "        logits = self.fc(logits)\n",
        "        \n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "ccdb26b8",
      "metadata": {
        "id": "ccdb26b8"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss(weight=class_weights, ignore_index=-1).to(DEVICE)\n",
        "\n",
        "model = NERModel(NUM_CLASSES, EMBEDDING_DIM, HIDDEN_DIM, initialize=True, fine_tune_embeddings=True, use_conv_embeddings=True).to(DEVICE)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "7754793c",
      "metadata": {
        "id": "7754793c"
      },
      "outputs": [],
      "source": [
        "def train_epoch(dataloader, model, optimizer, criterion, epoch):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    total_loss, total_batches = 0.0, 0.0\n",
        "    log_interval = 50\n",
        "\n",
        "    for idx, (sentences, labels, lengths, words) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "                        \n",
        "        logits = model(sentences, lengths, words)\n",
        "                           \n",
        "        N, L, _ = logits.shape\n",
        "        logits = logits.view(N * L, -1)\n",
        "        labels = labels.view(N * L)\n",
        "        loss = criterion(input=logits, target=labels)\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        total_batches += 1\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        \n",
        "        optimizer.step()\n",
        "        model.eval()\n",
        "\n",
        "        masks = (labels != -1)\n",
        "        total_acc += (logits.argmax(-1) == labels)[masks].sum().item()\n",
        "        total_count += masks.sum()\n",
        "\n",
        "        model.train()\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            print(f\"| epoch {epoch:3d} | {idx:5d}/{len(dataloader):5d} batches | accuracy {total_acc / total_count:8.3f} | loss {total_loss / total_batches:8.3f}\")\n",
        "            total_acc, total_count = 0, 0\n",
        "            total_loss, total_batches = 0.0, 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "e3cd7dc9",
      "metadata": {
        "id": "e3cd7dc9"
      },
      "outputs": [],
      "source": [
        "def evaluate(dataloader, model):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "    total_loss, total_batches = 0.0, 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (sentences, labels, lengths, words) in enumerate(dataloader):\n",
        "            logits = model(sentences, lengths, words)\n",
        "            N, L, _ = logits.shape\n",
        "            logits = logits.view(N * L, -1)\n",
        "            labels = labels.view(N * L)\n",
        "        \n",
        "            total_loss += criterion(input=logits, target=labels)\n",
        "            total_batches += 1\n",
        "        \n",
        "            masks = (labels != -1)\n",
        "            total_acc += (logits.argmax(-1) == labels)[masks].sum().item()\n",
        "            total_count += masks.sum()\n",
        "        \n",
        "    return total_acc / total_count, total_loss / total_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "bb9e1f47",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb9e1f47",
        "outputId": "f3da8951-e395-476b-938e-5f64bec14dd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch   1 |    50/  110 batches | accuracy    0.476 | loss    1.987\n",
            "| epoch   1 |   100/  110 batches | accuracy    0.766 | loss    1.481\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time: 16.12s | valid accuracy    0.776 | valid loss    1.223\n",
            "-----------------------------------------------------------\n",
            "| epoch   2 |    50/  110 batches | accuracy    0.768 | loss    1.201\n",
            "| epoch   2 |   100/  110 batches | accuracy    0.771 | loss    1.201\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time: 15.68s | valid accuracy    0.779 | valid loss    1.159\n",
            "-----------------------------------------------------------\n",
            "| epoch   3 |    50/  110 batches | accuracy    0.770 | loss    1.163\n",
            "| epoch   3 |   100/  110 batches | accuracy    0.771 | loss    1.168\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time: 16.92s | valid accuracy    0.780 | valid loss    1.159\n",
            "-----------------------------------------------------------\n",
            "| epoch   4 |    50/  110 batches | accuracy    0.772 | loss    1.160\n",
            "| epoch   4 |   100/  110 batches | accuracy    0.770 | loss    1.161\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time: 17.62s | valid accuracy    0.780 | valid loss    1.157\n",
            "-----------------------------------------------------------\n",
            "| epoch   5 |    50/  110 batches | accuracy    0.770 | loss    1.163\n",
            "| epoch   5 |   100/  110 batches | accuracy    0.773 | loss    1.157\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time: 16.57s | valid accuracy    0.780 | valid loss    1.156\n",
            "-----------------------------------------------------------\n",
            "| epoch   6 |    50/  110 batches | accuracy    0.774 | loss    1.165\n",
            "| epoch   6 |   100/  110 batches | accuracy    0.770 | loss    1.161\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   6 | time: 16.03s | valid accuracy    0.780 | valid loss    1.159\n",
            "-----------------------------------------------------------\n",
            "| epoch   7 |    50/  110 batches | accuracy    0.774 | loss    1.155\n",
            "| epoch   7 |   100/  110 batches | accuracy    0.770 | loss    1.161\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   7 | time: 17.99s | valid accuracy    0.780 | valid loss    1.159\n",
            "-----------------------------------------------------------\n",
            "| epoch   8 |    50/  110 batches | accuracy    0.771 | loss    1.162\n",
            "| epoch   8 |   100/  110 batches | accuracy    0.771 | loss    1.164\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   8 | time: 21.31s | valid accuracy    0.780 | valid loss    1.160\n",
            "-----------------------------------------------------------\n",
            "| epoch   9 |    50/  110 batches | accuracy    0.770 | loss    1.153\n",
            "| epoch   9 |   100/  110 batches | accuracy    0.773 | loss    1.165\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   9 | time: 19.95s | valid accuracy    0.780 | valid loss    1.163\n",
            "-----------------------------------------------------------\n",
            "| epoch  10 |    50/  110 batches | accuracy    0.772 | loss    1.161\n",
            "| epoch  10 |   100/  110 batches | accuracy    0.772 | loss    1.157\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  10 | time: 19.04s | valid accuracy    0.780 | valid loss    1.156\n",
            "-----------------------------------------------------------\n",
            "| epoch  11 |    50/  110 batches | accuracy    0.773 | loss    1.161\n",
            "| epoch  11 |   100/  110 batches | accuracy    0.770 | loss    1.161\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  11 | time: 21.71s | valid accuracy    0.780 | valid loss    1.159\n",
            "-----------------------------------------------------------\n",
            "| epoch  12 |    50/  110 batches | accuracy    0.773 | loss    1.147\n",
            "| epoch  12 |   100/  110 batches | accuracy    0.770 | loss    1.176\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  12 | time: 20.58s | valid accuracy    0.780 | valid loss    1.156\n",
            "-----------------------------------------------------------\n",
            "| epoch  13 |    50/  110 batches | accuracy    0.773 | loss    1.160\n",
            "| epoch  13 |   100/  110 batches | accuracy    0.769 | loss    1.169\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  13 | time: 20.83s | valid accuracy    0.780 | valid loss    1.155\n",
            "-----------------------------------------------------------\n",
            "| epoch  14 |    50/  110 batches | accuracy    0.772 | loss    1.168\n",
            "| epoch  14 |   100/  110 batches | accuracy    0.771 | loss    1.151\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  14 | time: 20.41s | valid accuracy    0.780 | valid loss    1.160\n",
            "-----------------------------------------------------------\n",
            "| epoch  15 |    50/  110 batches | accuracy    0.770 | loss    1.167\n",
            "| epoch  15 |   100/  110 batches | accuracy    0.772 | loss    1.152\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  15 | time: 19.71s | valid accuracy    0.780 | valid loss    1.152\n",
            "-----------------------------------------------------------\n",
            "| epoch  16 |    50/  110 batches | accuracy    0.772 | loss    1.158\n",
            "| epoch  16 |   100/  110 batches | accuracy    0.771 | loss    1.160\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  16 | time: 17.53s | valid accuracy    0.780 | valid loss    1.156\n",
            "-----------------------------------------------------------\n",
            "| epoch  17 |    50/  110 batches | accuracy    0.771 | loss    1.167\n",
            "| epoch  17 |   100/  110 batches | accuracy    0.773 | loss    1.153\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  17 | time: 17.79s | valid accuracy    0.780 | valid loss    1.154\n",
            "-----------------------------------------------------------\n",
            "| epoch  18 |    50/  110 batches | accuracy    0.772 | loss    1.164\n",
            "| epoch  18 |   100/  110 batches | accuracy    0.771 | loss    1.150\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  18 | time: 15.80s | valid accuracy    0.780 | valid loss    1.154\n",
            "-----------------------------------------------------------\n",
            "| epoch  19 |    50/  110 batches | accuracy    0.772 | loss    1.173\n",
            "| epoch  19 |   100/  110 batches | accuracy    0.771 | loss    1.148\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  19 | time: 19.57s | valid accuracy    0.780 | valid loss    1.155\n",
            "-----------------------------------------------------------\n",
            "| epoch  20 |    50/  110 batches | accuracy    0.770 | loss    1.160\n",
            "| epoch  20 |   100/  110 batches | accuracy    0.774 | loss    1.162\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  20 | time: 17.52s | valid accuracy    0.780 | valid loss    1.159\n",
            "-----------------------------------------------------------\n",
            "| epoch  21 |    50/  110 batches | accuracy    0.769 | loss    1.165\n",
            "| epoch  21 |   100/  110 batches | accuracy    0.774 | loss    1.156\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  21 | time: 16.06s | valid accuracy    0.780 | valid loss    1.159\n",
            "-----------------------------------------------------------\n",
            "| epoch  22 |    50/  110 batches | accuracy    0.772 | loss    1.154\n",
            "| epoch  22 |   100/  110 batches | accuracy    0.770 | loss    1.165\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  22 | time: 17.81s | valid accuracy    0.780 | valid loss    1.159\n",
            "-----------------------------------------------------------\n",
            "| epoch  23 |    50/  110 batches | accuracy    0.773 | loss    1.160\n",
            "| epoch  23 |   100/  110 batches | accuracy    0.769 | loss    1.163\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  23 | time: 18.64s | valid accuracy    0.780 | valid loss    1.160\n",
            "-----------------------------------------------------------\n",
            "| epoch  24 |    50/  110 batches | accuracy    0.772 | loss    1.158\n",
            "| epoch  24 |   100/  110 batches | accuracy    0.770 | loss    1.161\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  24 | time: 18.04s | valid accuracy    0.780 | valid loss    1.157\n",
            "-----------------------------------------------------------\n",
            "| epoch  25 |    50/  110 batches | accuracy    0.773 | loss    1.156\n",
            "| epoch  25 |   100/  110 batches | accuracy    0.771 | loss    1.155\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  25 | time: 16.85s | valid accuracy    0.780 | valid loss    1.161\n",
            "-----------------------------------------------------------\n",
            "Checking the results on test set...\n",
            "test accuracy    0.765 | test loss    1.161\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train_epoch(train_dataloader, model, optimizer, criterion, epoch)\n",
        "    accuracy, loss = evaluate(dev_dataloader, model)\n",
        "    scheduler.step()\n",
        "    print(\"-\" * 59)\n",
        "    print(f\"| end of epoch {epoch:3d} | time: {time.time() - epoch_start_time:5.2f}s | valid accuracy {accuracy:8.3f} | valid loss {loss:8.3f}\")\n",
        "    print(\"-\" * 59)\n",
        "\n",
        "print(\"Checking the results on test set...\")\n",
        "test_accuracy, test_loss = evaluate(test_dataloader, model)\n",
        "print(f\"test accuracy {test_accuracy:8.3f} | test loss {test_loss:8.3f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
